<!DOCTYPE html>
<html>
<head>
  <title>Gemini Audio-to-Audio WebSocket Demo</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <div class="input-container">
    <button id="micButton" onclick="toggleMicrophone()" disabled>Start Mic</button>
  </div>
  <div id="output"></div>

  <!-- Load EventEmitter3 first -->
  <script src="https://cdn.jsdelivr.net/npm/eventemitter3@5.0.1/dist/eventemitter3.umd.min.js"></script>

  <!-- Then load our module code -->
  <script type="module">
    import { AudioRecorder } from './audio-recorder.js';
    import { AudioStreamer } from './audio-streamer.js';

    const output = document.getElementById('output');
    const apiKey = '<YOUR_API_KEY>'; // Replace with your actual API key
    const host = 'generativelanguage.googleapis.com';
    const endpoint = `wss://${host}/ws/google.ai.generativelanguage.v1alpha.GenerativeService.BidiGenerateContent?key=${apiKey}`;

    const ws = new WebSocket(endpoint);
    let audioContext;
    let audioStreamer;
    let audioRecorder;
    let isRecording = false;
    let initialized = false;
    let isInterrupted = false;

    // Initialize audio when sending the first message
    async function ensureAudioInitialized() {
      if (!initialized) {
        audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 24000 });
        audioStreamer = new AudioStreamer(audioContext);
        await audioContext.resume();
        initialized = true;
        console.log('Audio context initialized:', audioContext.state);
      }
    }

    async function playAudioChunk(base64AudioChunk) {
      try {
        await ensureAudioInitialized();
        const arrayBuffer = base64ToArrayBuffer(base64AudioChunk);
        const uint8Array = new Uint8Array(arrayBuffer);
        audioStreamer.addPCM16(uint8Array);
        audioStreamer.resume();
      } catch (error) {
        console.error('Error queuing audio chunk:', error);
      }
    }

    async function startRecording() {
      try {
        await ensureAudioInitialized();

        // Reset state when starting new recording
        isInterrupted = false;
        audioStreamer.stop();  // Clean up any previous audio state

        audioRecorder = new AudioRecorder();
        await audioRecorder.start();

        audioRecorder.on('data', (base64Data) => {
          sendAudioChunk(base64Data);
        });

        logMessage('Recording started...');
        isRecording = true;
        document.getElementById('micButton').textContent = 'Stop Mic';
      } catch (error) {
        console.error('Error starting recording:', error);
        logMessage('Error starting recording: ' + error.message);
      }
    }

    function stopRecording() {
      if (audioRecorder) {
        audioRecorder.stop();
        audioRecorder.off('data');
        logMessage('Recording stopped.');
        isRecording = false;
        document.getElementById('micButton').textContent = 'Start Mic';
        
        // Reset interruption state when stopping recording
        isInterrupted = false;
        
        sendEndMessage();
      }
    }

    function sendAudioChunk(base64Audio) {
      const message = {
        realtime_input: {
          media_chunks: [{
            mime_type: "audio/pcm",
            data: base64Audio
          }]
        }
      };
      console.log("Sending audio message: ", message);
      ws.send(JSON.stringify(message));
    }

    function sendEndMessage() {
      const message = {
        client_content: {
          turns: [{
            role: "user",
            parts: []
          }],
          turn_complete: true
        }
      };
      ws.send(JSON.stringify(message));
    }

    function sendContinueSignal() {
      const message = {
        client_content: {
          turns: [{
            role: "user",
            parts: []
          }],
          turn_complete: false
        }
      };
      ws.send(JSON.stringify(message));
    }

    // Make toggleMicrophone available globally
    window.toggleMicrophone = function() {
      if (isRecording) {
        stopRecording();
      } else {
        startRecording();
      }
    };

    ws.onopen = () => {
      console.log('WebSocket connection is opening...');
      logMessage('Connected to Gemini');

      const setupMessage = {
        setup: {
          model: "models/gemini-2.0-flash-exp",
          generation_config: {
            response_modalities: ["audio"],
            speech_config: {
              voice_config: {
                prebuilt_voice_config: {
                  voice_name: "Aoede"  // One of: Aoede, Charon, Fenrir, Kore, Puck
                }
              }
            }
          }
        }
      };

      console.log('Sending setup message:', setupMessage);
      ws.send(JSON.stringify(setupMessage));
    };

    ws.onmessage = async (event) => {
      try {
        let wsResponse;
        if (event.data instanceof Blob) {
          const responseText = await event.data.text();
          wsResponse = JSON.parse(responseText);
        } else {
          wsResponse = JSON.parse(event.data);
        }

        console.log('WebSocket Response:', wsResponse);

        if (wsResponse.setupComplete) {
          document.getElementById('micButton').disabled = false;
        } else if (wsResponse.serverContent) {
          // Check for interruption
          if (wsResponse.serverContent.interrupted) {
            logMessage('Gemini: Interrupted');
            isInterrupted = true;
            audioStreamer.stop();
            return;
          }

          // Process audio data if present
          if (wsResponse.serverContent.modelTurn?.parts?.[0]?.inlineData) {
            const audioData = wsResponse.serverContent.modelTurn.parts[0].inlineData.data;
            if (!audioStreamer.isPlaying) {
              logMessage('Gemini: Speaking...');
            }
            await playAudioChunk(audioData);

            // Always send continue signal unless explicitly complete
            if (!wsResponse.serverContent.turnComplete) {
              sendContinueSignal();
            }
          }

          // Handle turn completion
          if (wsResponse.serverContent.turnComplete) {
            logMessage('Gemini: Finished speaking');
            isInterrupted = false;  // Reset interruption state
            audioStreamer.complete();
          }
        }
      } catch (error) {
        console.error('Error parsing response:', error);
        logMessage('Error parsing response: ' + error.message);
      }
    };

    function base64ToArrayBuffer(base64) {
      const binaryString = atob(base64);
      const bytes = new Uint8Array(binaryString.length);
      for (let i = 0; i < binaryString.length; i++) {
        bytes[i] = binaryString.charCodeAt(i);
      }
      return bytes.buffer;
    }

    ws.onerror = (error) => {
      console.error('WebSocket Error:', error);
      logMessage('WebSocket Error: ' + error.message);
    };

    ws.onclose = (event) => {
      console.log('Connection closed:', event);
      logMessage(`Connection closed - Code: ${event.code}, Reason: ${event.reason}`);
    };

    function logMessage(message) {
      const messageElement = document.createElement('p');
      messageElement.textContent = message;
      output.appendChild(messageElement);
    }
  </script>
</body>
</html>